{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def get_df(oos=True,domains=False) :\n",
    "    with open('data_full.json') as json_file: \n",
    "        data_dict = json.load(json_file) \n",
    "\n",
    "    train_data = data_dict['train']\n",
    "    val_data = data_dict['val']\n",
    "    test_data = data_dict['test']\n",
    "\n",
    "    oos_train = data_dict['oos_train']\n",
    "    oos_val = data_dict['oos_val']\n",
    "    oos_test = data_dict['oos_test']\n",
    "\n",
    "\n",
    "    train_df = pd.DataFrame(train_data, columns =['query', 'intent'])\n",
    "    val_df = pd.DataFrame(val_data, columns =['query', 'intent'])\n",
    "    test_df = pd.DataFrame(test_data, columns =['query', 'intent'])\n",
    "\n",
    "    train_oos_df = pd.DataFrame(oos_train,columns=['query','intent'])\n",
    "    val_oos_df = pd.DataFrame(oos_val,columns=['query','intent'])\n",
    "    test_oos_df = pd.DataFrame(oos_test,columns=['query','intent'])\n",
    "\n",
    "    if oos :\n",
    "        # Concatenate dataframes to consider oos as a specific intent\n",
    "        train_df = pd.concat([train_df,train_oos_df])\n",
    "        val_df = pd.concat([val_df,val_oos_df])\n",
    "        test_df = pd.concat([test_df,test_oos_df])\n",
    "    \n",
    "    train_df =pd.concat([train_df,val_df])\n",
    "\n",
    "    if domains:\n",
    "        with open('domains.json') as json_file:\n",
    "            domain_dict = json.load(json_file)\n",
    "        inv_domain_dict = {}\n",
    "        for domainKey in domain_dict.keys():\n",
    "            for intent in domain_dict[domainKey]:\n",
    "                inv_domain_dict[intent] = domainKey\n",
    "        if oos:\n",
    "            inv_domain_dict['oos']='oos'\n",
    "        train_df['domain'] = train_df.apply(lambda row: inv_domain_dict[row['intent']],axis=1)\n",
    "        test_df['domain'] = test_df.apply(lambda row: inv_domain_dict[row['intent']],axis=1)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "df_train, df_test = get_df(oos=False,domains=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "def utils_preprocess_text(text, flg_stemm=True, flg_lemm=False, lst_stopwords=None):\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming \n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation \n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "\n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "df_train[\"query_clean\"] = df_train[\"query\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True))\n",
    "df_test[\"query_clean\"] = df_test[\"query\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_as_lst(corpus):\n",
    "   ## create list of lists of unigrams\n",
    "   lst_corpus = []\n",
    "   for string in corpus:\n",
    "      lst_words = string.split()\n",
    "      lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, len(lst_words), 1)]\n",
    "      lst_corpus.append(lst_grams)\n",
    "   return lst_corpus\n",
    "\n",
    "# Prepare the corpus to be trained by Word2Vec\n",
    "train_corpus = corpus_as_lst(df_train['query_clean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training word embeddings\n",
    "wc_model = gensim.models.word2vec.Word2Vec(train_corpus, vector_size=300,   window=8, min_count=1, sg=1, epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_mean_vector(embeddings, text):\n",
    "    tokens = text.split()\n",
    "    vec = []\n",
    "    for i in range(len(tokens)):\n",
    "        try:\n",
    "            vec.append(embeddings.get_vector(tokens[i]))\n",
    "        except KeyError:\n",
    "            True   # simply ignore out-of-vocabulary tokens\n",
    "    if(len(vec)!=0):\n",
    "        return [sum([row[j] for row in vec]) / len(vec) for j in range(len(vec[0]))]\n",
    "    else : \n",
    "        return []\n",
    "\n",
    "def get_word_embdeddings(lst_corpus, model):\n",
    "    embeddings_corpus = []\n",
    "    for c in lst_corpus:\n",
    "        mean_vec = text_to_mean_vector(model.wv, c)\n",
    "        if(len(mean_vec)!=0):\n",
    "            embeddings_corpus.append(mean_vec)\n",
    "        else:\n",
    "            embeddings_corpus.append(np.zeros(model.wv.vector_size,))\n",
    "    return np.array(embeddings_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18200, 300)\n",
      "(5500, 300)\n"
     ]
    }
   ],
   "source": [
    "# Extracting word embeddings\n",
    "X_train = get_word_embdeddings(df_train['query_clean'], wc_model)\n",
    "X_test = get_word_embdeddings(df_test['query_clean'], wc_model)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_model(model,X_train,df_train,df_test,domains=False):\n",
    "        # Getting labels\n",
    "    if domains:\n",
    "        y_train = df_train['domain'].values\n",
    "        y_test = df_test['domain'].values\n",
    "    else:\n",
    "        y_train = df_train['intent'].values\n",
    "        y_test = df_test['intent'].values\n",
    "        \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    return y_pred,y_test\n",
    "\n",
    "# Training \n",
    "lr = LogisticRegression(multi_class='multinomial', max_iter=300)\n",
    "y_pred,y_test=train_model(lr,X_train,df_train,df_test,domains=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix shape: (11, 11)\n",
      "Accuracy 0.7796363636363637\n",
      "Precision 0.796376966149679\n",
      "Recall 0.8502626262626264\n",
      "F1 0.789790700868814\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "print('Confusion matrix shape:',confusion_matrix(y_test, y_pred).shape)\n",
    "\n",
    "# accuracy, precision, recall, f1\n",
    "\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "precision = precision_score(y_test,y_pred,average='macro')\n",
    "recall = recall_score(y_test,y_pred,average='macro')\n",
    "f1 = f1_score(y_test,y_pred,average='macro')\n",
    "print('Accuracy',accuracy)\n",
    "print('Precision',precision)\n",
    "print('Recall',recall)\n",
    "print('F1',f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvpy3107",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
