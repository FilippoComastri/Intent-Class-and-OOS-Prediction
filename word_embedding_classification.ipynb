{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION USING WORD EMBEDDINGS\n",
    "\n",
    "Togheter with sparse representations we tried to use dense representation techniques using word embedding. We tried two different approach : the first one training our own word embeddings using the training set we had, and the second one using pre-trained word embeddings.\n",
    "\n",
    "First of all we worked a little bit on the dataset. We aggregate training and validation set to have more data for training, and we keep test set to evaluate the performance of the model.\n",
    "We also considered different scenarios :\n",
    "- considering or not the Out-Of-Scope samples \n",
    "- using intents or domains as labels for classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *get_df(oos,domains)* creates the dataframe used for training , grouping by domains or not and considering or not the Out-of-Scope samples. In all cases it merges togheter validation and train set into the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def get_df(oos=False,domains=False) :\n",
    "    with open('data_full.json') as json_file: \n",
    "        data_dict = json.load(json_file) \n",
    "\n",
    "    train_data = data_dict['train']\n",
    "    val_data = data_dict['val']\n",
    "    test_data = data_dict['test']\n",
    "\n",
    "    oos_train = data_dict['oos_train']\n",
    "    oos_val = data_dict['oos_val']\n",
    "    oos_test = data_dict['oos_test']\n",
    "\n",
    "\n",
    "    train_df = pd.DataFrame(train_data, columns =['query', 'intent'])\n",
    "    val_df = pd.DataFrame(val_data, columns =['query', 'intent'])\n",
    "    test_df = pd.DataFrame(test_data, columns =['query', 'intent'])\n",
    "\n",
    "    train_oos_df = pd.DataFrame(oos_train,columns=['query','intent'])\n",
    "    val_oos_df = pd.DataFrame(oos_val,columns=['query','intent'])\n",
    "    test_oos_df = pd.DataFrame(oos_test,columns=['query','intent'])\n",
    "\n",
    "    if oos :\n",
    "        # Concatenate dataframes to consider oos as a specific intent\n",
    "        train_df = pd.concat([train_df,train_oos_df])\n",
    "        val_df = pd.concat([val_df,val_oos_df])\n",
    "        test_df = pd.concat([test_df,test_oos_df])\n",
    "    \n",
    "    train_df =pd.concat([train_df,val_df])\n",
    "\n",
    "    if domains:\n",
    "        with open('domains.json') as json_file:\n",
    "            domain_dict = json.load(json_file)\n",
    "        inv_domain_dict = {}\n",
    "        for domainKey in domain_dict.keys():\n",
    "            for intent in domain_dict[domainKey]:\n",
    "                inv_domain_dict[intent] = domainKey\n",
    "        if oos:\n",
    "            inv_domain_dict['oos']='oos'\n",
    "        train_df['domain'] = train_df.apply(lambda row: inv_domain_dict[row['intent']],axis=1)\n",
    "        test_df['domain'] = test_df.apply(lambda row: inv_domain_dict[row['intent']],axis=1)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "df_train, df_test = get_df(oos=True,domains=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING\n",
    "\n",
    "In the first step we pre-process our data. This step included only stemming or lemmatization, togheter with the elimination of stop words. Other pre-processing operations are not necessary because our dataset contains sentences already downcased and without special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "def utils_preprocess_text(text, flg_stemm=True, flg_lemm=False, lst_stopwords=None):\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming \n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation \n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "\n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "df_train[\"query_clean\"] = df_train[\"query\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True))\n",
    "df_test[\"query_clean\"] = df_test[\"query\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING WORD EMBEDDINGS\n",
    "\n",
    "\n",
    "We use Word2Vec as model for training word embeddings and in particular we use the implementation given by the Gensim library.\n",
    "\n",
    "To train our word embeddings using the train set we need to transform each row of the corpus into a list of unigrams. This is done by the function *corpus_as_lst(corpus)* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_as_lst(corpus):\n",
    "   ## create list of lists of unigrams\n",
    "   lst_corpus = []\n",
    "   for string in corpus:\n",
    "      lst_words = string.split()\n",
    "      lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, len(lst_words), 1)]\n",
    "      lst_corpus.append(lst_grams)\n",
    "   return lst_corpus\n",
    "\n",
    "# Prepare the corpus to be trained by Word2Vec\n",
    "train_corpus = corpus_as_lst(df_train['query_clean'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we feed the Word2Vec model with the corpus transformed into a list of lists. We specify some parameters , in particular :\n",
    "- *vector_size* is the size of each word embedding vector.\n",
    "- *window* is the size of the window of words near the target word, that are implicitly considered as positive.\n",
    "- *sg=1* indicates that we want to use skip-gram as training algorithm.\n",
    "- *min_count=1* ignores all words with total frequency lower than 1.\n",
    "- *epochs* is the number of iterations over the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training word embeddings\n",
    "wc_model = gensim.models.word2vec.Word2Vec(train_corpus, vector_size=300,   window=8, min_count=1, sg=1, epochs=30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training our word embeddings we can also use pre-trained word embeddings. We use **word2vec-google-news-300** pre-trained vectors which are trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load pretrained word embeddings\n",
    "google_300 = gensim.models.KeyedVectors.load(\"./models/google_300\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the word embeddings, we want to transform each sentence into a word vector. This is done using the **text_to_mean(embeddings,text)** function which takes as inputs the word embeddings and a sentence. \n",
    "\n",
    "First of all it splits into tokens the sentence and for each token searches for the corresponding word vector. If the word is not present into the word embdeddings vocabulary it simply ignores it. \n",
    "\n",
    "At this point we have a NxM matrix where N is the number of words composing the sentence and M is the size of word vector (in our case 300). \n",
    "\n",
    "But we want to obtain only one vector for each sentence. So we simply take the mean along columns and we return the corresponding vector.\n",
    "\n",
    "The function *get_word_embeddings(corpus)* simply computes the *text_to_mean* function to all sentences of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_mean_vector(embeddings, text):\n",
    "    tokens = text.split()\n",
    "    vec = []\n",
    "    for i in range(len(tokens)):\n",
    "        try:\n",
    "            vec.append(embeddings.get_vector(tokens[i]))\n",
    "        except KeyError:\n",
    "            True   # simply ignore out-of-vocabulary tokens\n",
    "    if(len(vec)!=0):\n",
    "        return [sum([row[j] for row in vec]) / len(vec) for j in range(len(vec[0]))]\n",
    "    else : \n",
    "        return []  # if every token of the sentence is out-of-vocabulary we simply return an empty list\n",
    "\n",
    "def get_word_embdeddings(corpus, model):\n",
    "    embeddings_corpus = []\n",
    "    for c in corpus:\n",
    "        mean_vec = text_to_mean_vector(model, c)\n",
    "        if(len(mean_vec)!=0):\n",
    "            embeddings_corpus.append(mean_vec)\n",
    "        else:\n",
    "            embeddings_corpus.append(np.zeros(model.vector_size,)) # if every token of the sentence is out-of-vocabulary we represents that sentence as a list of zeros\n",
    "    return np.array(embeddings_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18200, 300)\n",
      "(5500, 300)\n"
     ]
    }
   ],
   "source": [
    "# Extracting word embeddings\n",
    "X_train = get_word_embdeddings(df_train['query_clean'], google_300)\n",
    "X_test = get_word_embdeddings(df_test['query_clean'], google_300)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_model(model,X_train,df_train,df_test,domains=False):\n",
    "        # Getting labels\n",
    "    if domains:\n",
    "        y_train = df_train['domain'].values\n",
    "        y_test = df_test['domain'].values\n",
    "    else:\n",
    "        y_train = df_train['intent'].values\n",
    "        y_test = df_test['intent'].values\n",
    "        \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    return y_pred,y_test\n",
    "\n",
    "# Training \n",
    "lr = LogisticRegression(multi_class='multinomial', max_iter=300)\n",
    "y_pred,y_test=train_model(lr,X_train,df_train,df_test,domains=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           precision    recall  f1-score   support\n",
      "\n",
      "                translate       0.96      0.87      0.91        30\n",
      "                 transfer       0.72      0.87      0.79        30\n",
      "                    timer       0.97      0.97      0.97        30\n",
      "               definition       0.81      1.00      0.90        30\n",
      "          meaning_of_life       0.90      0.93      0.92        30\n",
      "         insurance_change       0.97      0.97      0.97        30\n",
      "               find_phone       0.79      0.90      0.84        30\n",
      "             travel_alert       0.68      0.70      0.69        30\n",
      "              pto_request       0.81      0.87      0.84        30\n",
      "     improve_credit_score       0.88      1.00      0.94        30\n",
      "                 fun_fact       0.88      1.00      0.94        30\n",
      "          change_language       0.57      0.80      0.67        30\n",
      "                   payday       0.61      0.67      0.63        30\n",
      "replacement_card_duration       0.70      0.93      0.80        30\n",
      "                     time       0.91      0.97      0.94        30\n",
      "       application_status       0.66      0.83      0.74        30\n",
      "            flight_status       0.94      1.00      0.97        30\n",
      "                flip_coin       0.75      0.90      0.82        30\n",
      "         change_user_name       1.00      0.97      0.98        30\n",
      "       where_are_you_from       0.97      0.93      0.95        30\n",
      "     shopping_list_update       0.90      0.90      0.90        30\n",
      "       what_can_i_ask_you       0.70      0.77      0.73        30\n",
      "                    maybe       0.85      0.97      0.91        30\n",
      "           oil_change_how       0.83      0.97      0.89        30\n",
      "   restaurant_reservation       0.69      0.80      0.74        30\n",
      "                  balance       1.00      0.77      0.87        30\n",
      "      confirm_reservation       0.88      0.97      0.92        30\n",
      "           freeze_account       0.76      0.83      0.79        30\n",
      "            rollover_401k       1.00      0.87      0.93        30\n",
      "             who_made_you       0.88      0.97      0.92        30\n",
      "                 distance       0.91      0.97      0.94        30\n",
      "                user_name       0.55      0.87      0.68        30\n",
      "                 timezone       0.80      0.80      0.80        30\n",
      "                next_song       0.78      0.93      0.85        30\n",
      "             transactions       0.73      0.90      0.81        30\n",
      "    restaurant_suggestion       0.83      1.00      0.91        30\n",
      "          rewards_balance       0.50      1.00      0.67        30\n",
      "                 pay_bill       0.61      0.67      0.63        30\n",
      "         spending_history       0.88      1.00      0.94        30\n",
      "       pto_request_status       0.90      0.87      0.88        30\n",
      "             credit_score       0.90      0.90      0.90        30\n",
      "                 new_card       0.76      0.87      0.81        30\n",
      "             lost_luggage       0.86      1.00      0.92        30\n",
      "                   repeat       1.00      1.00      1.00        30\n",
      "                      mpg       0.79      0.90      0.84        30\n",
      "          oil_change_when       0.90      0.90      0.90        30\n",
      "                      yes       0.77      1.00      0.87        30\n",
      "        travel_suggestion       0.80      0.93      0.86        30\n",
      "                insurance       0.81      0.97      0.88        30\n",
      "         todo_list_update       0.66      0.77      0.71        30\n",
      "                 reminder       0.67      0.87      0.75        30\n",
      "             change_speed       0.81      0.73      0.77        30\n",
      "            tire_pressure       0.79      0.90      0.84        30\n",
      "                       no       0.87      0.87      0.87        30\n",
      "                      apr       0.51      0.87      0.64        30\n",
      "           nutrition_info       0.81      1.00      0.90        30\n",
      "                 calendar       0.71      0.80      0.75        30\n",
      "                     uber       0.97      1.00      0.98        30\n",
      "               calculator       0.81      0.83      0.82        30\n",
      "                     date       0.76      0.87      0.81        30\n",
      "                 carry_on       0.91      1.00      0.95        30\n",
      "                 pto_used       0.91      1.00      0.95        30\n",
      "     schedule_maintenance       0.88      1.00      0.94        30\n",
      "      travel_notification       0.83      1.00      0.91        30\n",
      "              sync_device       0.85      0.97      0.91        30\n",
      "                thank_you       0.62      0.80      0.70        30\n",
      "                roll_dice       0.79      0.90      0.84        30\n",
      "                food_last       0.63      0.87      0.73        30\n",
      "                cook_time       0.90      0.90      0.90        30\n",
      "          reminder_update       0.87      0.90      0.89        30\n",
      "         report_lost_card       0.84      0.87      0.85        30\n",
      "  ingredient_substitution       0.86      0.83      0.85        30\n",
      "                make_call       0.72      0.87      0.79        30\n",
      "                    alarm       0.62      0.87      0.72        30\n",
      "                todo_list       0.69      0.97      0.81        30\n",
      "            change_accent       0.97      0.93      0.95        30\n",
      "                       w2       0.79      0.73      0.76        30\n",
      "                 bill_due       0.94      1.00      0.97        30\n",
      "                 calories       0.79      1.00      0.88        30\n",
      "             damaged_card       0.93      0.93      0.93        30\n",
      "       restaurant_reviews       0.91      0.35      0.51      1000\n",
      "                  routing       0.48      0.73      0.58        30\n",
      "         do_you_have_pets       0.85      0.93      0.89        30\n",
      "         schedule_meeting       0.71      0.80      0.75        30\n",
      "                 gas_type       0.87      0.87      0.87        30\n",
      "                plug_type       0.79      0.73      0.76        30\n",
      "              tire_change       0.97      0.93      0.95        30\n",
      "            exchange_rate       0.79      0.77      0.78        30\n",
      "             next_holiday       0.90      0.93      0.92        30\n",
      "            change_volume       0.84      0.87      0.85        30\n",
      "      who_do_you_work_for       0.87      0.87      0.87        30\n",
      "             credit_limit       0.90      0.93      0.92        30\n",
      "                 how_busy       0.75      0.90      0.82        30\n",
      "      accept_reservations       0.60      0.80      0.69        30\n",
      "             order_status       0.74      0.77      0.75        30\n",
      "               pin_change       0.83      0.83      0.83        30\n",
      "                  goodbye       0.67      0.80      0.73        30\n",
      "          account_blocked       0.87      0.90      0.89        30\n",
      "                what_song       0.78      0.83      0.81        30\n",
      "       international_fees       0.93      0.83      0.88        30\n",
      "         last_maintenance       0.83      0.80      0.81        30\n",
      "         meeting_schedule       1.00      0.97      0.98        30\n",
      "         ingredients_list       0.77      0.90      0.83        30\n",
      "             report_fraud       0.75      0.80      0.77        30\n",
      "   measurement_conversion       0.72      0.77      0.74        30\n",
      "               smart_home       0.74      0.87      0.80        30\n",
      "               book_hotel       1.00      0.97      0.98        30\n",
      "         current_location       1.00      0.97      0.98        30\n",
      "                  weather       0.86      1.00      0.92        30\n",
      "                    taxes       0.76      0.87      0.81        30\n",
      "              min_payment       0.79      1.00      0.88        30\n",
      "             whisper_mode       0.79      0.87      0.83        30\n",
      "                   cancel       0.67      0.73      0.70        30\n",
      "       international_visa       0.70      0.77      0.73        30\n",
      "                 vaccines       0.46      0.90      0.61        30\n",
      "              pto_balance       0.63      0.87      0.73        30\n",
      "               directions       0.79      0.90      0.84        30\n",
      "                 spelling       0.81      1.00      0.90        30\n",
      "                 greeting       0.83      1.00      0.91        30\n",
      "           reset_settings       0.81      0.97      0.88        30\n",
      "        what_is_your_name       0.77      0.90      0.83        30\n",
      "           direct_deposit       0.72      0.87      0.79        30\n",
      "            interest_rate       0.69      0.90      0.78        30\n",
      "      credit_limit_change       0.83      0.97      0.89        30\n",
      "    what_are_your_hobbies       1.00      0.90      0.95        30\n",
      "              book_flight       0.96      0.90      0.93        30\n",
      "            shopping_list       0.91      1.00      0.95        30\n",
      "                     text       0.54      0.87      0.67        30\n",
      "             bill_balance       0.76      0.73      0.75        30\n",
      "           share_location       0.74      0.87      0.80        30\n",
      "           redeem_rewards       0.72      0.77      0.74        30\n",
      "               play_music       0.93      0.93      0.93        30\n",
      "          calendar_update       0.93      0.90      0.92        30\n",
      "            are_you_a_bot       0.81      0.87      0.84        30\n",
      "                      gas       0.88      0.97      0.92        30\n",
      "          expiration_date       0.46      0.87      0.60        30\n",
      "          update_playlist       0.91      0.97      0.94        30\n",
      "       cancel_reservation       0.97      1.00      0.98        30\n",
      "                tell_joke       0.68      0.87      0.76        30\n",
      "           change_ai_name       0.97      0.97      0.97        30\n",
      "          how_old_are_you       0.81      1.00      0.90        30\n",
      "               car_rental       0.59      0.97      0.73        30\n",
      "               jump_start       0.74      0.77      0.75        30\n",
      "          meal_suggestion       0.53      0.80      0.64        30\n",
      "                   recipe       0.71      0.80      0.75        30\n",
      "                   income       0.83      0.97      0.89        30\n",
      "                    order       0.76      0.97      0.85        30\n",
      "                  traffic       0.97      1.00      0.98        30\n",
      "             order_checks       0.78      0.70      0.74        30\n",
      "            card_declined       0.71      0.90      0.79        30\n",
      "                      oos       0.58      0.47      0.52        30\n",
      "\n",
      "                 accuracy                           0.79      5500\n",
      "                macro avg       0.80      0.89      0.84      5500\n",
      "             weighted avg       0.82      0.79      0.78      5500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = df_test['intent'].unique()\n",
    "print(classification_report(y_test,y_pred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix shape: (151, 151)\n",
      "Accuracy 0.7985454545454546\n",
      "Precision 0.8012696503229092\n",
      "Recall 0.8890242825607063\n",
      "F1 0.8369035053953743\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "print('Confusion matrix shape:',confusion_matrix(y_test, y_pred).shape)\n",
    "\n",
    "# accuracy, precision, recall, f1\n",
    "\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "precision = precision_score(y_test,y_pred,average='macro')\n",
    "recall = recall_score(y_test,y_pred,average='macro')\n",
    "f1 = f1_score(y_test,y_pred,average='macro')\n",
    "print('Accuracy',accuracy)\n",
    "print('Precision',precision)\n",
    "print('Recall',recall)\n",
    "print('F1',f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvpy3107",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
