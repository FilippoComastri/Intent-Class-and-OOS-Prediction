{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data into representations that Pandas can handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('data_full.json') as json_file: \n",
    "    data_dict = json.load(json_file)\n",
    "\n",
    "train = data_dict['train']\n",
    "val = data_dict['val']\n",
    "test = data_dict['test']\n",
    "\n",
    "oos_train = data_dict['oos_train']\n",
    "oos_val = data_dict['oos_val']\n",
    "oos_test = data_dict['oos_test']\n",
    "\n",
    "\n",
    "train_val_df = pd.DataFrame(train + val, columns =['query', 'intent'])\n",
    "test_df = pd.DataFrame(test, columns =['query', 'intent'])\n",
    "\n",
    "train_val_oos_df = pd.DataFrame(train + val + oos_train + oos_val, columns =['query', 'intent'])\n",
    "test_oos_df = pd.DataFrame(test + oos_test, columns =['query', 'intent'])\n",
    "\n",
    "# Collecting intent->domain mappings for later analysis.\n",
    "with open('domains.json') as json_file:\n",
    "    domain_dict = json.load(json_file)\n",
    "inv_domain_dict = {k:v for v,klist in domain_dict.items() for k in klist}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing datasets for training, merging train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_train = train_val_df['query']\n",
    "y_train_intents = train_val_df['intent']\n",
    "y_train_domains = train_val_df['intent'].map(inv_domain_dict)\n",
    "sents_test = test_df['query']\n",
    "y_test_intents = test_df['intent']\n",
    "y_test_domains = test_df['intent'].map(inv_domain_dict)\n",
    "\n",
    "# Creating a separate dataset that includes 'out-of-scope' as an other intent.\n",
    "inv_domain_dict_oos = inv_domain_dict.copy()\n",
    "inv_domain_dict_oos['oos'] = 'oos'\n",
    "sents_train_oos = train_val_oos_df['query']\n",
    "y_train_oos_domains = train_val_oos_df['intent'].map(inv_domain_dict_oos)\n",
    "sents_test_oos = test_oos_df['query']\n",
    "y_test_oos_domains = test_oos_df['intent'].map(inv_domain_dict_oos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction using bag of words and TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_BOF = count_vectorizer.fit_transform(sents_train).toarray()\n",
    "X_test_BOF = count_vectorizer.transform(sents_test).toarray()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_TFIDF = tfidf_vectorizer.fit_transform(sents_train).toarray()\n",
    "X_test_TFIDF = tfidf_vectorizer.transform(sents_test).toarray()\n",
    "\n",
    "count_vectorizer_oos = CountVectorizer()\n",
    "X_train_oos_BOF = count_vectorizer_oos.fit_transform(sents_train_oos).toarray()\n",
    "X_test_oos_BOF = count_vectorizer_oos.transform(sents_test_oos).toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for evaluating classifier preformance by printing scores in 4 different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def print_res(classifier, X_test, y_test):\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1:\", f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some preliminary experiments showed that among the basic models, logistic regression is the fastest at training while retaining pretty high performance (according to the scores). Two tasks (inent and intent domain prediction), two feature extraction methods (Bag of Words and TF-IDF) and two datasets (without or with out-of-scope entries) were tested during the expreiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent prediction, bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.908\n",
      "Precision: 0.9124231868458526\n",
      "Recall: 0.9080000000000001\n",
      "F1: 0.9078813647351022\n"
     ]
    }
   ],
   "source": [
    "clf_log_int_BOF = LogisticRegression()\n",
    "clf_log_int_BOF.fit(X_train_BOF, y_train_intents)\n",
    "print_res(clf_log_int_BOF, X_test_BOF, y_test_intents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domain prediction, bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.954\n",
      "Precision: 0.9545715463290531\n",
      "Recall: 0.9540000000000001\n",
      "F1: 0.9540870037465297\n"
     ]
    }
   ],
   "source": [
    "clf_log_dom_BOF = LogisticRegression(max_iter=300)\n",
    "clf_log_dom_BOF.fit(X_train_BOF, y_train_domains)\n",
    "print_res(clf_log_dom_BOF, X_test_BOF, y_test_domains)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent prediction, TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9004444444444445\n",
      "Precision: 0.9055589994843706\n",
      "Recall: 0.9004444444444446\n",
      "F1: 0.8998474385369699\n"
     ]
    }
   ],
   "source": [
    "clf_log_int_TFIDF = LogisticRegression()\n",
    "clf_log_int_TFIDF.fit(X_train_TFIDF, y_train_intents)\n",
    "print_res(clf_log_int_TFIDF, X_test_TFIDF, y_test_intents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domain prediction, TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9535555555555556\n",
      "Precision: 0.9542313025907816\n",
      "Recall: 0.9535555555555556\n",
      "F1: 0.9536453047160508\n"
     ]
    }
   ],
   "source": [
    "clf_log_dom_TFIDF = LogisticRegression(max_iter=300)\n",
    "clf_log_dom_TFIDF.fit(X_train_TFIDF, y_train_domains)\n",
    "print_res(clf_log_dom_TFIDF, X_test_TFIDF, y_test_domains)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domain prediction with OOS, Bag of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8029090909090909\n",
      "Precision: 0.8189435571270347\n",
      "Recall: 0.8780101010101009\n",
      "F1: 0.812591250960203\n"
     ]
    }
   ],
   "source": [
    "clf_oos_log_dom_BOF = LogisticRegression(max_iter=300)\n",
    "clf_oos_log_dom_BOF.fit(X_train_oos_BOF, y_train_oos_domains)\n",
    "print_res(clf_oos_log_dom_BOF, X_test_oos_BOF, y_test_oos_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# clf = MultinomialNB()\n",
    "# clf.fit(X_train_BOF, y_train_intents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple code to test one of the models on user-given data. (With Bag of Words feature extraction.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence given:  Can you decide if this question belongs to oos or not?\n",
      "Domain prediction:  meta\n",
      "Domain prediction (oos included):  meta\n"
     ]
    }
   ],
   "source": [
    "sentence = input(\"Write in a sentence:\")\n",
    "sentence_vec = count_vectorizer.transform([sentence]).toarray()\n",
    "sentence_vec_oos = count_vectorizer_oos.transform([sentence]).toarray()\n",
    "print(\"Sentence given: \", sentence)\n",
    "print(\"Domain prediction: \", clf_log_dom_BOF.predict(sentence_vec)[0])\n",
    "print(\"Domain prediction (oos included): \", clf_oos_log_dom_BOF.predict(sentence_vec_oos)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
